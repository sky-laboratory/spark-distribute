{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 19:02:51 WARN Utils: Your hostname, imhaneul-ui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.75 instead (on interface en0)\n",
      "23/02/09 19:02:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 19:02:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ml_taxi\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "trip_file = f\"{os.getcwd()}/data/trips\"\n",
    "trip_directory = spark.read.parquet(f\"{trip_file}/*\")\n",
    "trip_directory.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_directory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=========================================================(11 + 0) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|year|       y|\n",
      "+----+--------+\n",
      "|2003|       2|\n",
      "|2004|       1|\n",
      "|2009|     137|\n",
      "|2020|       1|\n",
      "|2021|15000794|\n",
      "|2029|       1|\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qs = \"\"\"\n",
    "SELECT \n",
    "    YEAR(pickup_date) as year,\n",
    "    count(*) as y\n",
    "FROM \n",
    "    (\n",
    "        SELECT \n",
    "            split(tpep_pickup_datetime, \" \")[0] as pickup_date\n",
    "        FROM \n",
    "            trips\n",
    "    )\n",
    "GROUP BY \n",
    "    year\n",
    "ORDER BY \n",
    "    year\n",
    "\"\"\"\n",
    "spark.sql(qs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|   passenger_count|    trip_distance|      total_amount|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|          14119430|         14951326|          14951326|\n",
      "|   mean|1.4249466161169395|6.622507772889028|18.751955821150126|\n",
      "| stddev|1.0441240925988922|671.6468121191299|145.98221322318227|\n",
      "|    min|               0.0|              0.0|            -647.8|\n",
      "|    max|               9.0|        332541.19|          398469.2|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qs = \"\"\"\n",
    "SELECT \n",
    "    passenger_count, \n",
    "    tpep_pickup_datetime as pickup_datetime,\n",
    "    trip_distance,\n",
    "    total_amount\n",
    "FROM \n",
    "    trips\n",
    "WHERE \n",
    "    TO_DATE(tpep_pickup_datetime) >= \"2021-01-01\"\n",
    "    AND TO_DATE(tpep_pickup_datetime) < \"2021-08-01\" \n",
    "\"\"\"\n",
    "spark.sql(qs).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = \"\"\"\n",
    "SELECT \n",
    "    HOUR(tpep_pickup_datetime) as pickup_datetime,\n",
    "    DATE_FORMAT(tpep_pickup_datetime, \"EEEE\") as day_of_week,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    passenger_count, \n",
    "    trip_distance,\n",
    "    total_amount\n",
    "FROM \n",
    "    trips\n",
    "WHERE\n",
    "    passenger_count BETWEEN 1 AND 4\n",
    "    AND trip_distance BETWEEN 3 AND 500\n",
    "    AND total_amount BETWEEN 1 AND 500\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= \"2021-01-01\"\n",
    "    AND TO_DATE(tpep_pickup_datetime) < \"2021-08-01\" \n",
    "\"\"\"\n",
    "data = spark.sql(qs)\n",
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|summary|   pickup_datetime|day_of_week|pickup_location_id|dropoff_location_id|   passenger_count|    trip_distance|      total_amount|\n",
      "+-------+------------------+-----------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "|  count|           3304180|    3304180|           3304180|            3304180|           3304180|          3304180|           3304180|\n",
      "|   mean|10.491263490487807|       null|157.06760013074347|  151.8498998238595|1.3031553971030634|7.083868442397225| 32.27237469821481|\n",
      "| stddev| 8.030924492452023|       null| 64.88764715005858|  77.59881161292482|0.6323994705499705|5.704137940945662|18.072610395017485|\n",
      "|    min|                 0|     Friday|                 1|                  1|               1.0|              3.0|              1.25|\n",
      "|    max|                23|  Wednesday|               265|                265|               4.0|            474.1|            499.05|\n",
      "+-------+------------------+-----------+------------------+-------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data.randomSplit([0.8, 0.2], seed=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data count => 2644102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:==============================================>          (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data count => 660078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"train data count => {train_df.count()}\")\n",
    "print(f\"test data count => {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categorical_feature = [\n",
    "    'pickup_location_id',\n",
    "    'dropoff_location_id',\n",
    "    'day_of_week'\n",
    "]\n",
    "stages = []\n",
    "\n",
    "for c in categorical_feature:\n",
    "    categori_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot = OneHotEncoder(inputCols=[categori_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    stages += [categori_indexer, onehot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_527ae13b6bd3,\n",
       " OneHotEncoder_4370edbcb5e2,\n",
       " StringIndexer_e6993219d885,\n",
       " OneHotEncoder_554255c400ed,\n",
       " StringIndexer_81cf8a1b0afd,\n",
       " OneHotEncoder_e0e1ac1bf801]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "num = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_datetime\"\n",
    "]\n",
    "for n in num:\n",
    "    num_vectorasem = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    num_scaler = StandardScaler(inputCol=num_vectorasem.getOutputCol(), outputCol=n+\"_scaler\")\n",
    "    stages += [num_vectorasem, num_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_527ae13b6bd3,\n",
       " OneHotEncoder_4370edbcb5e2,\n",
       " StringIndexer_e6993219d885,\n",
       " OneHotEncoder_554255c400ed,\n",
       " StringIndexer_81cf8a1b0afd,\n",
       " OneHotEncoder_e0e1ac1bf801,\n",
       " VectorAssembler_04039e28de58,\n",
       " StandardScaler_bce6db924278,\n",
       " VectorAssembler_dfdffa0e0b47,\n",
       " StandardScaler_0cc0c36aad9f,\n",
       " VectorAssembler_ef20e6efad86,\n",
       " StandardScaler_a276e8d460d7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featrue_assembler = [c+\"_onehot\" for c in categorical_feature] + [n+\"_scaler\" for n in num]\n",
    "assem = VectorAssembler(inputCols=featrue_assembler, outputCol=\"features_vector\")\n",
    "stages += [assem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_527ae13b6bd3,\n",
       " OneHotEncoder_4370edbcb5e2,\n",
       " StringIndexer_e6993219d885,\n",
       " OneHotEncoder_554255c400ed,\n",
       " StringIndexer_81cf8a1b0afd,\n",
       " OneHotEncoder_e0e1ac1bf801,\n",
       " VectorAssembler_04039e28de58,\n",
       " StandardScaler_bce6db924278,\n",
       " VectorAssembler_dfdffa0e0b47,\n",
       " StandardScaler_0cc0c36aad9f,\n",
       " VectorAssembler_ef20e6efad86,\n",
       " StandardScaler_a276e8d460d7,\n",
       " VectorAssembler_787e1d034f75]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipleline = Pipeline(stages=stages)\n",
    "fitted_tranformer = pipleline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_tranformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "  maxIter=50,\n",
    "  regParam=0.01,\n",
    "  labelCol=\"total_amount\",\n",
    "  featuresCol=\"features_vector\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:====================>                                    (4 + 7) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 19:04:19 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/09 19:04:19 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 19:04:22 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7542036698485332"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.944717741486938"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_df = fitted_tranformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------------+-------------+------------------+------------+\n",
      "|pickup_datetime|day_of_week|passenger_count|trip_distance|        prediction|total_amount|\n",
      "+---------------+-----------+---------------+-------------+------------------+------------+\n",
      "|              0|     Friday|            4.0|         8.36|33.225443593531075|       40.56|\n",
      "|              0|     Friday|            1.0|          6.2| 29.46179441133163|        29.0|\n",
      "|              0|     Friday|            1.0|         3.32| 23.16769415528741|        17.3|\n",
      "|              0|     Friday|            1.0|          3.0| 22.44842437838726|        17.3|\n",
      "|              0|     Friday|            1.0|          3.0| 22.44842437838726|       21.36|\n",
      "|              0|     Friday|            1.0|         4.44|25.585195881745527|        21.3|\n",
      "|              0|     Friday|            1.0|          8.5| 35.99321470875731|        28.3|\n",
      "|              0|     Friday|            1.0|          8.0| 40.93223985937816|        25.3|\n",
      "|              0|     Friday|            1.0|          3.8|24.731281732102097|       26.16|\n",
      "|              0|     Friday|            1.0|         3.68|22.919340207219026|       29.12|\n",
      "|              0|     Friday|            2.0|         5.69|28.533001749036767|        26.0|\n",
      "|              0|     Friday|            2.0|         4.87|27.242668979867528|        31.8|\n",
      "|              0|     Friday|            1.0|          5.5|26.999324240542904|        25.3|\n",
      "|              0|     Friday|            1.0|          4.8| 26.68901895086902|        22.8|\n",
      "|              0|     Friday|            1.0|         9.43|35.467090699671125|       39.96|\n",
      "|              0|     Friday|            2.0|          9.5| 36.57643329775769|        36.8|\n",
      "|              0|     Friday|            1.0|         3.05| 24.40950333202263|        19.3|\n",
      "|              0|     Friday|            1.0|          3.2|23.584671329565836|        16.8|\n",
      "|              0|     Friday|            1.0|         3.66| 24.45679076897166|       19.56|\n",
      "|              0|     Friday|            1.0|          3.9|   26.112155603674|       25.56|\n",
      "+---------------+-----------+---------------+-------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pred = model.transform(vtest_df)\n",
    "pred.select(\"pickup_datetime\", \"day_of_week\", \"passenger_count\", \"trip_distance\", \"prediction\", \"total_amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2516ec9dedd577fb6662378dc80c75bb748f265cb700a702aa389fbd4cb2ebee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
